{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Stanford Sentiment Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Di Bai, Yipeng He, and Zijian Wang\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Methodological note](#Methodological-note)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [A softmax baseline](#A-softmax-baseline)\n",
    "1. [RNNClassifier wrapper](#RNNClassifier-wrapper)\n",
    "1. [Error analysis](#Error-analysis)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Sentiment words alone [2 points]](#Sentiment-words-alone-[2-points])\n",
    "  1. [A more powerful vector-summing baseline [3 points]](#A-more-powerful-vector-summing-baseline-[3-points])\n",
    "  1. [Your original system [4 points]](#Your-original-system-[4-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This homework and associated bake-off are devoted to the Stanford Sentiment Treebank (SST). The homework questions ask you to implement some baseline systems, and the bake-off challenge is to define a system that does extremely well at the SST task.\n",
    "\n",
    "We'll focus on the ternary task as defined by `sst.ternary_class_func`.\n",
    "\n",
    "The SST test set will be used for the bake-off evaluation. This dataset is already publicly distributed, so we are counting on people not to cheat by develping their models on the test set. You must do all your development without using the test set at all, and then evaluate exactly once on the test set and turn in the results, with no further system tuning or additional runs. __Much of the scientific integrity of our field depends on people adhering to this honor code__. \n",
    "\n",
    "Our only additional restriction is that __you cannot make any use of the subtree labels__. This corresponds to the 'Root' condition in the paper. As we discussed in class, the subtree labels are a really interesting feature of SST, but bringing them in results in a substantially different learning problem.\n",
    "\n",
    "One of our goals for this homework and bake-off is to encourage you to engage in __the basic development cycle for supervised models__, in which you\n",
    "\n",
    "1. Write a new feature function. We recommend starting with something simple.\n",
    "1. Use `sst.experiment` to evaluate your new feature function, with at least `fit_softmax_classifier`.\n",
    "1. If you have time, compare your feature function with `unigrams_phi` using `sst.compare_models` or `sst.compare_models_mcnemar`. (For discussion, see [this notebook section](sst_02_hand_built_features.ipynb#Statistical-comparison-of-classifier-models).)\n",
    "1. Return to step 1, or stop the cycle and conduct a more rigorous evaluation with hyperparameter tuning and assessment on the `dev` set.\n",
    "\n",
    "[Error analysis](#Error-analysis) is one of the most important methods for steadily improving a system, as it facilitates a kind of human-powered hill-climbing on your ultimate objective. Often, it takes a careful human analyst just a few examples to spot a major pattern that can lead to a beneficial change to the feature representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodological note\n",
    "\n",
    "You don't have to use the experimental framework defined below (based on `sst`). However, if you don't use `sst.experiment` as below, then make sure you're training only on `train`, evaluating on `dev`, and that you report with \n",
    "\n",
    "```\n",
    "from sklearn.metrics import classification_report\n",
    "classification_report(y_dev, predictions)\n",
    "```\n",
    "where `y_dev = [y for tree, y in sst.dev_reader(class_func=sst.ternary_class_func)]`. We'll focus on the value at `macro avg` under `f1-score` in these reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](sst_01_overview.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sst\n",
    "import torch.nn as nn\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_HOME = os.path.join('data', 'trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A softmax baseline\n",
    "\n",
    "This example is here mainly as a reminder of how to use our experimental framework with linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(tree):\n",
    "    \"\"\"The basis for a unigrams feature function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : nltk.tree\n",
    "        The tree to represent.\n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    Counter\n",
    "        A map from strings to their counts in `tree`. (Counter maps a \n",
    "        list to a dict of counts of the elements in that list.)\n",
    "    \n",
    "    \"\"\"\n",
    "    return Counter(tree.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thin wrapper around `LogisticRegression` for the sake of `sst.experiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_classifier(X, y):        \n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='ovr')\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental run with some notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.628     0.689     0.657       428\n",
      "     neutral      0.343     0.153     0.211       229\n",
      "    positive      0.629     0.750     0.684       444\n",
      "\n",
      "   micro avg      0.602     0.602     0.602      1101\n",
      "   macro avg      0.533     0.531     0.518      1101\n",
      "weighted avg      0.569     0.602     0.575      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax_experiment = sst.experiment(\n",
    "    SST_HOME,\n",
    "    unigrams_phi,                      # Free to write your own!\n",
    "    fit_softmax_classifier,            # Free to write your own!\n",
    "    train_reader=sst.train_reader,     # Fixed by the competition.\n",
    "    assess_reader=sst.dev_reader,      # Fixed until the bake-off.\n",
    "    class_func=sst.ternary_class_func) # Fixed by the bake-off rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax_experiment` contains a lot of information that you can use for analysis; see [this section below](#Error-analysis) for starter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNClassifier wrapper\n",
    "\n",
    "This section illustrates how to use `sst.experiment` with RNN and TreeNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To featurize examples for an RNN, we just get the words in order, letting the model take care of mapping them into an embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_phi(tree):\n",
    "    return tree.leaves()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model wrapper gets the vocabulary using `sst.get_vocab`. If you want to use pretrained word representations in here, then you can have `fit_rnn_classifier` build that space too; see [this notebook section for details](sst_03_neural_networks.ipynb#Pretrained-embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rnn_classifier(X, y):    \n",
    "    sst_glove_vocab = utils.get_vocab(X, n_words=10000)     \n",
    "    mod = TorchRNNClassifier(\n",
    "        sst_glove_vocab, \n",
    "        eta=0.05,\n",
    "        embedding=None,\n",
    "        batch_size=1000,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=50,\n",
    "        max_iter=50,\n",
    "        l2_strength=0.001,\n",
    "        bidirectional=True,\n",
    "        hidden_activation=nn.ReLU())\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 2.2622278034687042"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.603     0.631     0.616       428\n",
      "     neutral      0.261     0.214     0.235       229\n",
      "    positive      0.634     0.664     0.649       444\n",
      "\n",
      "   micro avg      0.558     0.558     0.558      1101\n",
      "   macro avg      0.499     0.503     0.500      1101\n",
      "weighted avg      0.544     0.558     0.550      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_experiment = sst.experiment(\n",
    "    SST_HOME,\n",
    "    rnn_phi,\n",
    "    fit_rnn_classifier, \n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_reader=sst.dev_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "This section begins to build an error-analysis framework using the dicts returned by `sst.experiment`. These have the following structure:\n",
    "\n",
    "```\n",
    "'model': trained model\n",
    "'train_dataset':\n",
    "   'X': feature matrix\n",
    "   'y': list of labels\n",
    "   'vectorizer': DictVectorizer,\n",
    "   'raw_examples': list of raw inputs, before featurizing   \n",
    "'assess_dataset': same structure as the value of 'train_dataset'\n",
    "'predictions': predictions on the assessment data\n",
    "'metric': `score_func.__name__`, where `score_func` is an `sst.experiment` argument\n",
    "'score': the `score_func` score on the assessment data\n",
    "```\n",
    "The following function just finds mistakes, and returns a `pd.DataFrame` for easy subsequent processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(experiment):\n",
    "    \"\"\"Find mistaken predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment : dict\n",
    "        As returned by `sst.experiment`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    raw_examples = experiment['assess_dataset']['raw_examples']\n",
    "    raw_examples = [\" \".join(tree.leaves()) for tree in raw_examples]\n",
    "    df = pd.DataFrame({\n",
    "        'raw_examples': raw_examples,\n",
    "        'predicted': experiment['predictions'],\n",
    "        'gold': experiment['assess_dataset']['y']})\n",
    "    df['correct'] = df['predicted'] == df['gold']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_analysis = find_errors(softmax_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_analysis = find_errors(rnn_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we merge the sotmax and RNN experiments into a single DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = softmax_analysis.merge(\n",
    "    rnn_analysis, left_on='raw_examples', right_on='raw_examples')\n",
    "\n",
    "analysis = analysis.drop('gold_y', axis=1).rename(columns={'gold_x': 'gold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code collects a specific subset of examples; small modifications to its structure will give you different interesting subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples where the softmax model is correct, the RNN is not,\n",
    "# and the gold label is 'positive'\n",
    "\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] == analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] != analysis['gold'])    \n",
    "    &\n",
    "    (analysis['gold'] == 'positive')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_group.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "The magic of the film lies not in the mysterious spring but in the richness of its performances .\n",
      "======================================================================\n",
      "My Wife Is an Actress is an utterly charming French comedy that feels so American in sensibility and style it 's virtually its own Hollywood remake .\n",
      "======================================================================\n",
      "It haunts you , you ca n't forget it , you admire its conception and are able to resolve some of the confusions you had while watching it .\n",
      "======================================================================\n",
      "The film may appear naked in its narrative form ... but it goes deeper than that , to fundamental choices that include the complexity of the Catholic doctrine\n",
      "======================================================================\n",
      "Jose Campanella delivers a loosely autobiographical story brushed with sentimentality but brimming with gentle humor , bittersweet pathos , and lyric moments that linger like snapshots of memory .\n"
     ]
    }
   ],
   "source": [
    "for ex in error_group['raw_examples'].sample(5):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment words alone [2 points]\n",
    "\n",
    "NLTK includes an easy interface to [Minqing Hu and Bing Liu's __Opinion Lexicon__](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), which consists of a list of positive words and a list of negative words. How much of the ternary SST story does this lexicon tell?\n",
    "\n",
    "For this problem, submit code to do the following:\n",
    "\n",
    "1. Create a feature function `op_unigrams` on the model of `unigrams_phi` above, but filtering the vocabulary to just items that are members of the Opinion Lexicon. Submit this feature function.\n",
    "\n",
    "1. Evaluate your feature function with `sst.experiment`, with all the same parameters as were used to create `softmax_experiment` in [A softmax baseline](#A-softmax-baseline) above, except of course for the feature function.\n",
    "\n",
    "1. Use `utils.mcnemar` to compare your feature function with the results in `softmax_experiment`. The information you need for this is in `softmax_experiment` and your own `sst.experiment` results. Submit your evaluation code. You can assume `softmax_experiment` is already in memory, but your code should create the other objects necessary for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "# Use set for fast membership checking:\n",
    "positive = set(opinion_lexicon.positive())\n",
    "negative = set(opinion_lexicon.negative())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_unigrams(tree):\n",
    "    vocabulary = list(filter(lambda x: x in positive or x in negative, tree.leaves()))\n",
    "    return Counter(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.553     0.752     0.638       428\n",
      "     neutral      0.179     0.031     0.052       229\n",
      "    positive      0.615     0.664     0.639       444\n",
      "\n",
      "   micro avg      0.567     0.567     0.567      1101\n",
      "   macro avg      0.449     0.482     0.443      1101\n",
      "weighted avg      0.500     0.567     0.516      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op_softmax_experiment = sst.experiment(\n",
    "    SST_HOME,\n",
    "    op_unigrams,                      \n",
    "    fit_softmax_classifier,            # Free to write your own!\n",
    "    train_reader=sst.train_reader,     # Fixed by the competition.\n",
    "    assess_reader=sst.dev_reader,      # Fixed until the bake-off.\n",
    "    class_func=sst.ternary_class_func) # Fixed by the bake-off rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.328413284132841, 0.020980477345314247)\n"
     ]
    }
   ],
   "source": [
    "m = utils.mcnemar(\n",
    "    softmax_experiment['assess_dataset']['y'], \n",
    "    op_softmax_experiment['predictions'],\n",
    "    softmax_experiment['predictions'])\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more powerful vector-summing baseline [3 points]\n",
    "\n",
    "In [Distributed representations as features](sst_03_neural_networks.ipynb#Distributed-representations-as-features), we looked at a baseline for the ternary SST problem in which each example is modeled as the sum of its 50-dimensional GloVe representations. A `LogisticRegression` model was used for prediction. A neural network might do better here, since there might be complex relationships between the input feature dimensions that a linear classifier can't learn. \n",
    "\n",
    "To address this question, rerun the experiment with `torch_shallow_neural_classifier.TorchShallowNeuralClassifier` as the classifier. Specs:\n",
    "* Use `sst.experiment` to conduct the experiment. \n",
    "* Using 3-fold cross-validation, exhaustively explore this set of hyperparameter combinations:\n",
    "  * The hidden dimensionality at 50, 100, and 200.\n",
    "  * The hidden activation function as `nn.Tanh` or `nn.ReLU`.\n",
    "* (For all other parameters to `TorchShallowNeuralClassifier`, use the defaults.)\n",
    "\n",
    "For this problem, submit code to do the following:\n",
    "\n",
    "1. Your model wrapper function around `TorchShallowNeuralClassifier`. This function should implement the requisite cross-validation; see [this notebook section](sst_02_hand_built_features.ipynb#Hyperparameter-search) for examples.\n",
    "1. Your average F1 score according to `sst.experiment`. \n",
    "2. The optimal hyperparameters chosen in your experiment. (You can just paste in the dict that `sst._experiment` prints.)\n",
    "\n",
    "We're not evaluating the quality of your model. (We've specified the protocols completely, but there will still be a  lot of variation in the results.) However, the primary goal of this question is to get you thinking more about this strikingly good baseline feature representation scheme for SST, so we're sort of hoping you feel compelled to try out variations on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "DATE_HOME = 'data'\n",
    "GLOVE_HOME = os.path.join(DATE_HOME, 'glove.6B')\n",
    "\n",
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))\n",
    "\n",
    "def vsm_leaves_phi(tree, lookup, np_func=np.sum):\n",
    "    allvecs = np.array([lookup[w] for w in tree.leaves() if w in lookup])    \n",
    "    if len(allvecs) == 0:\n",
    "        dim = len(next(iter(lookup.values())))\n",
    "        feats = np.zeros(dim)\n",
    "    else:       \n",
    "        feats = np_func(allvecs, axis=0)      \n",
    "    return feats\n",
    "\n",
    "def glove_leaves_phi(tree, np_func=np.sum):\n",
    "    return vsm_leaves_phi(tree, glove_lookup, np_func=np_func)\n",
    "\n",
    "def fit_nn_classifier(X, y):   \n",
    "    basemod = TorchShallowNeuralClassifier()\n",
    "    cv = 3\n",
    "    param_grid = {'hidden_dim': [50, 100, 200], 'hidden_activation': [nn.Tanh(), nn.ReLU()]}\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(X, y, basemod, cv, param_grid)\n",
    "    return best_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 2.6160860359668732"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'hidden_activation': Tanh(), 'hidden_dim': 50}\n",
      "Best score: 0.515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.594     0.688     0.638      1004\n",
      "     neutral      0.289     0.181     0.223       491\n",
      "    positive      0.686     0.702     0.694      1069\n",
      "\n",
      "   micro avg      0.597     0.597     0.597      2564\n",
      "   macro avg      0.523     0.524     0.518      2564\n",
      "weighted avg      0.574     0.597     0.582      2564\n",
      "\n",
      "0.5181095156690892\n"
     ]
    }
   ],
   "source": [
    "nn_experiment = sst.experiment(\n",
    "    SST_HOME,\n",
    "    glove_leaves_phi,\n",
    "    fit_nn_classifier,\n",
    "    class_func=sst.ternary_class_func,\n",
    "    score_func=utils.safe_macro_f1,\n",
    "    vectorize=False)  \n",
    "\n",
    "print(nn_experiment['score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [4 points]\n",
    "\n",
    "Your task is to develop an original model for the SST ternary problem. There are many options. If you spend more than a few hours on this homework problem, you should consider letting it grow into your final project! Here are some relatively manageable ideas that you might try:\n",
    "\n",
    "1. We didn't systematically evaluate the `bidirectional` option to the `TorchRNNClassifier`. Similarly, that model could be tweaked to allow multiple LSTM layers (at present there is only one), and you could try adding layers to the classifier portion of the model as well.\n",
    "\n",
    "1. We've already glimpsed the power of rich initial word representations, and later in the course we'll see that smart initialization usually leads to a performance gain in NLP, so you could perhaps achieve a winning entry with a simple model that starts in a great place.\n",
    "\n",
    "1. The [practical introduction to contextual word representations](contextualreps.ipynb) (to be discussed later in the quarter) covers pretrained representations and interfaces that are likely to boost the performance of any system.\n",
    "\n",
    "1. The `TreeNN` and `TorchTreeNN` don't perform all that well, and this could be for the same reason that RNNs don't peform well: the gradient signal doesn't propagate reliably down inside very deep trees. [Tai et al. 2015](https://aclanthology.info/papers/P15-1150/p15-1150) sought to address this with TreeLSTMs, which are fairly easy to implement in PyTorch.\n",
    "\n",
    "1. In the [distributed representations as features](#Distributed-representations-as-features) section, we just summed  all of the leaf-node GloVe vectors to obtain a fixed-dimensional representation for all sentences. This ignores all of the tree structure. See if you can do better by paying attention to the binary tree structure: write a function `glove_subtree_phi` that obtains a vector representation for each subtree by combining the vectors of its daughters, with the leaf nodes again given by GloVe (any dimension you like) and the full representation of the sentence given by the final vector obtained by this recursive process. You can decide on how you combine the vectors. \n",
    "\n",
    "1. If you have a lot of computing resources, then you can fire off a large hyperparameter search over many parameter values. All the model classes for this course are compatible with the `scikit-learn` and [scikit-optimize](https://scikit-optimize.github.io) methods, because they define the required functions for getting and setting parameters.\n",
    "\n",
    "We want to emphasize that this needs to be an __original__ system. It doesn't suffice to download code from the Web, retrain, and submit. You can build on others' code, but you have to do something new and meaningful with it.\n",
    "\n",
    "__Please include a brief prose description of your system along with your code, to help the teaching team understand the structure of your system.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Methods:\n",
    " >- The basic idea is to fine-tune the *whole* BERT model, instead of using `BERT-as-service`. We found that this yields a better performance for this task.\n",
    "> - We first preprocess the data, specifically:\n",
    ">  * We balance the dataset by oversampling\n",
    ">  * We fix the issue with the naive sentence joining function (see `sent_filter`)\n",
    "> - We then fine-tune the model using the [pretrained BERT model in PyTorch](https://github.com/huggingface/pytorch-pretrained-BERT) by Hugging face. A decent amount of this implementation was from the source code there.\n",
    ">  * We adopt some of the recommended grid search settings in the original BERT paper.\n",
    " \n",
    ">## Code:\n",
    "> - In the following block, we define helper functions (mostly from the repo linked above). We provide another block for evaluation using the model dump, and a final (commented) block for the training script. Everything should be on GPU by default, but you can run the initialization block and evaluation block with CPU.\n",
    "> - Additional packages needed were shown in the following block. You may run `pip install pytorch-pretrained-bert` and `pip install imbalanced-learn`.\n",
    "> - You will need to download the pretrained model at [here](https://drive.google.com/file/d/1BBForAuU7BzyhGZs_r7JuHXn6y3PpVt6/view?usp=sharing) using Stanford account. Unzip it so that the model file would be at `./models/pytorch_model.bin`. Otherwise, you could run the training block, which takes a few hours on 4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization: Re-initialize SST dataset\n",
    "def sent_filter(sent):\n",
    "    return sent.replace(\" 's\", \"'s\").replace(\" .\", \".\").replace(\" ,\", \",\").replace(\"`` \", \"'\") \\\n",
    "            .replace(\" ''\", \"'\").replace(\" 'm\", \"'m\").replace(\" 've\", \"'ve\") \\\n",
    "            .replace(\" 't\", \"'t\").replace(\" 're\", \"'re\") \n",
    "sst_train_reader = sst.train_reader(SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_train = [(sent_filter(\" \".join(t.leaves())),label) for t, label in sst_train_reader]\n",
    "sst_dev_reader = sst.dev_reader(SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_dev = [(sent_filter(\" \".join(t.leaves())), label) for t, label in sst_dev_reader]\n",
    "sst_test_reader = sst.test_reader(SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_test = [(sent_filter(\" \".join(t.leaves())), label) for t, label in sst_test_reader]\n",
    "\n",
    "\n",
    "## Additional imports\n",
    "import csv, os, json, random, logging, sys, pickle, time\n",
    "from tqdm import trange, tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from collections import *\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from bert_serving.client import BertClient \n",
    "from sklearn.metrics import classification_report\n",
    "import imblearn\n",
    "\n",
    "## Helpers\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "    \n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "class CondProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.label = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\n",
    "        pass\n",
    "        \n",
    "    def get_train_examples(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(sst_train, \"train\")\n",
    "\n",
    "    def get_dev_examples(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(sst_dev, \"dev\")\n",
    "    \n",
    "    def get_test_examples(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(sst_test, \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\"]\n",
    "\n",
    "    def _create_examples(self, data, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        i = 0\n",
    "        \n",
    "        sents = np.array([d[0] for d in data])\n",
    "        labels = np.array([d[1] for d in data])\n",
    "        \n",
    "        if set_type == \"train\":\n",
    "            logging.info(\"getting oversampled train data\")\n",
    "            ros = RandomOverSampler(random_state=42)\n",
    "            X_res, y_res = ros.fit_resample(np.arange(len(sents)).reshape(-1, 1), labels)\n",
    "            X_res = sents[X_res.reshape(-1)]\n",
    "            logging.info(Counter(y_res))\n",
    "        else:\n",
    "            logging.info(\"getting dev/test data\")\n",
    "            X_res = sents\n",
    "            y_res = labels\n",
    "\n",
    "        for e in zip(X_res, y_res):\n",
    "            text_a = e[0]\n",
    "            text_b = None\n",
    "            label = self.label[e[1]]\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            i += 1\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "      \n",
    "        return examples\n",
    "\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, show_exp=False):\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[str(example.label)]\n",
    "        if ex_index < 5 and show_exp:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Evaluation: Evaluate using best model\n",
    "args = dotdict({\"data_dir\": './data/bert/', \n",
    "                \"bert_model\": \"bert-large-cased\",\n",
    "                \"output_dir\": \"./models/\",\n",
    "                \"model_save_pth\": \"./models/bert_classification.pth\",\n",
    "                \"seed\": 28,\n",
    "                \"train_batch_size\": 30,\n",
    "                \"num_train_epochs\": 8,\n",
    "                \"eval_batch_size\": 30,\n",
    "                \"do_lower_case\": False,\n",
    "                \"do_train\": True,\n",
    "                \"do_eval\": True,\n",
    "                \"max_seq_length\": 100,\n",
    "                \"gradient_accumulation_steps\": 1,\n",
    "                \"local_rank\": -1,\n",
    "                \"warmup_proportion\": 0.1,\n",
    "                \"fp16\": False,\n",
    "                \"cache_dir\": \"./tmp/\",\n",
    "                \"learning_rate\": 9E-6,\n",
    "                \"do_train_eval\": False})\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349\n",
    "logger.info(\"device is \" + str(device))\n",
    "n_gpu = torch.cuda.device_count()\n",
    "args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "processor = CondProcessor()\n",
    "label_list = processor.get_labels()\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "num_labels = 3\n",
    "\n",
    "# Prepare model\n",
    "model = BertForSequenceClassification.from_pretrained(args.bert_model,\n",
    "          cache_dir=args.cache_dir,\n",
    "          num_labels = num_labels)\n",
    "model.load_state_dict(torch.load('models/pytorch_model.bin', map_location=device))\n",
    "logging.info(\"loaded best model\")\n",
    "\n",
    "eval_examples = processor.get_dev_examples() # change this line to `get_test_examples` for test dataset evaluation\n",
    "eval_features = convert_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "    labels.append(label_ids)\n",
    "    preds.append(pred)\n",
    "\n",
    "# f1 = f1_score(np.concatenate(labels), np.concatenate(preds), average=\"macro\")\n",
    "# logging.info(\"*** Dev F1: %s\" % f1)\n",
    "# logging.info(\"\")\n",
    "print(classification_report(np.concatenate(labels), np.concatenate(preds), digits=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## System Training Log\n",
    ">### Best args\n",
    "     {'data_dir': './data/bert/', 'bert_model': 'bert-large-cased', 'output_dir': './models/', 'model_save_pth': './models/bert_classification.pth', 'seed': 28, 'train_batch_size': 30, 'num_train_epochs': 8, 'eval_batch_size': 90, 'do_lower_case': False, 'do_train': True, 'do_eval': True, 'max_seq_length': 100, 'gradient_accumulation_steps': 1, 'task_name': 'test', 'local_rank': -1, 'warmup_proportion': 0.1, 'fp16': False, 'cache_dir': './tmp/', 'learning_rate': 9e-06, 'do_train_eval': False} \n",
    ">### Log\n",
    ">```\n",
    "04/22/2019 01:10:08 - INFO -   tr loss: 82.70523658394814\n",
    "04/22/2019 01:10:08 - INFO -   do eval\n",
    "04/22/2019 01:10:08 - INFO -   getting dev data\n",
    "04/22/2019 01:10:25 - INFO -   F1 0.6934594278565237\n",
    "04/22/2019 01:10:25 - INFO -   Best F1 0.6934594278565237\n",
    "04/22/2019 01:10:25 - INFO -   ***** Eval results *****\n",
    "04/22/2019 01:10:25 - INFO -     eval_f1 = 0.6934594278565237\n",
    "04/22/2019 01:10:25 - INFO -     eval_loss = 11.088425636291504\n",
    "04/22/2019 01:10:25 - INFO -     global_step = 1083\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Train\n",
    "# best_f1 = 0\n",
    "# for learning_rate in [5E-6, 9E-6, 1E-5, 2E-5]:\n",
    "#         args = dotdict({\"data_dir\": './data/bert/', \n",
    "#                 \"bert_model\": \"bert-large-cased\",\n",
    "#                 \"output_dir\": \"./models/\",\n",
    "#                 \"model_save_pth\": \"./models/bert_classification.pth\",\n",
    "#                 \"seed\": 28,\n",
    "#                 \"train_batch_size\": 30,\n",
    "#                 \"num_train_epochs\": 8,\n",
    "#                 \"eval_batch_size\": 90,\n",
    "#                 \"do_lower_case\": False,\n",
    "#                 \"do_train\": True,\n",
    "#                 \"do_eval\": True,\n",
    "#                 \"max_seq_length\": 100,\n",
    "#                 \"gradient_accumulation_steps\": 1,\n",
    "#                 \"task_name\": \"test\",\n",
    "#                 \"local_rank\": -1,\n",
    "#                 \"warmup_proportion\": 0.1,\n",
    "#                 \"fp16\": False,\n",
    "#                 \"cache_dir\": \"./tmp/\",\n",
    "#                 \"learning_rate\": learning_rate,\n",
    "#                 \"do_train_eval\": False})\n",
    "\n",
    "\n",
    "\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "#         logging.info(args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         processor = CondProcessor()\n",
    "#         label_list = processor.get_labels()\n",
    "\n",
    "#         tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#         num_labels = 3\n",
    "#         train_examples = None\n",
    "#         num_train_optimization_steps = None\n",
    "#         if args.do_train:\n",
    "#             train_examples = processor.get_train_examples()\n",
    "#             num_train_optimization_steps = int(\n",
    "#                 len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "#             if args.local_rank != -1:\n",
    "#                 num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "#         # Prepare model\n",
    "#         cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n",
    "#         model = BertForSequenceClassification.from_pretrained(args.bert_model,\n",
    "#                   cache_dir=cache_dir,\n",
    "#                   num_labels = num_labels)\n",
    "\n",
    "#         model = torch.nn.DataParallel(model)\n",
    "#         model.to(device)\n",
    "\n",
    "\n",
    "#         param_optimizer = list(model.named_parameters())\n",
    "#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#             {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#             ]\n",
    "\n",
    "\n",
    "#         optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "#                              lr=args.learning_rate,\n",
    "#                              warmup=args.warmup_proportion,\n",
    "#                              t_total=num_train_optimization_steps)\n",
    "\n",
    "#         train_f1s = []\n",
    "#         eval_f1s = []\n",
    "#         global_step = 0\n",
    "#         nb_tr_steps = 0\n",
    "#         tr_loss = 0\n",
    "#         patience = 0\n",
    "#         if args.do_train:\n",
    "\n",
    "\n",
    "#             for _ in trange(int(args.num_train_epochs)):\n",
    "\n",
    "#                 logger.info(\"do train\")\n",
    "\n",
    "#                 train_features = convert_examples_to_features(\n",
    "#                 train_examples, label_list, args.max_seq_length, tokenizer)\n",
    "#                 all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "#                 all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "#                 all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "#                 all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "#                 train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "#                 if args.local_rank == -1:\n",
    "#                     train_sampler = RandomSampler(train_data)\n",
    "#                 else:\n",
    "#                     train_sampler = DistributedSampler(train_data)\n",
    "#                 train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last = True, pin_memory=True)\n",
    "\n",
    "\n",
    "#                 model.train()\n",
    "#                 tr_loss = 0\n",
    "#                 nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "#                 for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "\n",
    "#                     batch = tuple(t.to(device, non_blocking=True) for t in batch)\n",
    "\n",
    "#                     input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "#                     loss = model(input_ids, segment_ids, input_mask, label_ids) #???\n",
    "#                     if n_gpu > 1:\n",
    "#                         loss = loss.mean() # mean() to average on multi-gpu.\n",
    "#                     loss.backward()\n",
    "\n",
    "#                     tr_loss += loss.item()\n",
    "#                     nb_tr_examples += input_ids.size(0)\n",
    "#                     nb_tr_steps += 1\n",
    "#                     optimizer.step()\n",
    "#                     optimizer.zero_grad()\n",
    "#                     global_step += 1\n",
    "#                     del input_ids, input_mask, segment_ids, label_ids, batch, loss\n",
    "\n",
    "#                 logger.info(\"tr loss: %s\" % tr_loss)\n",
    "\n",
    "          \n",
    "\n",
    "\n",
    "#                 if args.do_eval:\n",
    "#                     logger.info(\"do eval\")\n",
    "\n",
    "#                     eval_examples = processor.get_dev_examples()\n",
    "#                     eval_features = convert_examples_to_features(\n",
    "#                         eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "#                     all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "#                     all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "#                     all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "#                     all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "#                     eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "#                     # Run prediction for full data\n",
    "#                     eval_sampler = SequentialSampler(eval_data)\n",
    "#                     eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "#                     model.eval()\n",
    "#                     eval_loss = 0\n",
    "#                     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "#                     preds = []\n",
    "#                     labels = []\n",
    "#                     for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "#                         input_ids = input_ids.to(device)\n",
    "#                         input_mask = input_mask.to(device)\n",
    "#                         segment_ids = segment_ids.to(device)\n",
    "#                         label_ids = label_ids.to(device)\n",
    "\n",
    "#                         with torch.no_grad():\n",
    "#                             tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "#                             logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "#                         logits = logits.detach().cpu().numpy()\n",
    "#                         label_ids = label_ids.to('cpu').numpy()\n",
    "#                         pred = np.argmax(logits, axis=1)\n",
    "#                         labels.append(label_ids)\n",
    "#                         preds.append(pred)\n",
    "#                         eval_loss += tmp_eval_loss.mean().item()\n",
    "\n",
    "#                         nb_eval_examples += input_ids.size(0)\n",
    "#                         nb_eval_steps += 1\n",
    "#                         del input_ids, input_mask, segment_ids, label_ids, tmp_eval_loss\n",
    "\n",
    "#                     f1 = f1_score(np.concatenate(labels), np.concatenate(preds), average=\"macro\")\n",
    "#                     logger.info(\"F1 %s\" % f1)\n",
    "#                     eval_f1s.append(f1)\n",
    "#                     if f1 > best_f1:\n",
    "\n",
    "#                         best_f1 = f1\n",
    "#                         assert type(eval_loss) == float\n",
    "#                         logger.info(\"Best F1 %s\" % best_f1)\n",
    "#                         result = {'eval_loss': eval_loss,\n",
    "#                                   'eval_f1': f1,\n",
    "#                                   'global_step': global_step}\n",
    "\n",
    "#                         output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "#                         with open(output_eval_file, \"w\") as writer:\n",
    "#                             logger.info(\"***** Eval results *****\")\n",
    "#                             for key in sorted(result.keys()):\n",
    "#                                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#                                 writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "#                         model.to(device)\n",
    "#                         model_to_save = model.module if hasattr(model, 'module') else model \n",
    "#                         output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n",
    "#                         torch.save(model_to_save.state_dict(), output_model_file)\n",
    "#                         output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n",
    "#                         with open(output_config_file, 'w') as f:\n",
    "#                             f.write(model_to_save.config.to_json_string())\n",
    "#                         output_param_file = os.path.join(args.output_dir, \"param\")\n",
    "#                         with open(output_param_file, 'w') as f:\n",
    "#                             json.dump(args.__dict__, f, indent=2, sort_keys=True)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "The bake-off will begin on April 22. The announcement will go out on Piazza. As we said above, the bake-off evaluation data is the official SST test set release. For this bake-off, you'll evaluate your original system from the above homework problem on the test set, using the ternary class problem. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "To enter the bake-off, upload this notebook on Canvas:\n",
    "\n",
    "https://canvas.stanford.edu/courses/99711/assignments/187246\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "Systems that enter will receive the additional homework point, and systems that achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "The bake-off will close at 4:30 pm on April 24. Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2019 16:17:19 - INFO -   device is cpu\n",
      "04/22/2019 16:17:19 - INFO -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /Users/baidi/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "04/22/2019 16:17:20 - INFO -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz from cache at ./tmp/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "04/22/2019 16:17:20 - INFO -   extracting archive file ./tmp/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233 to temp dir /var/folders/cf/lwl2s47102j6p475ny0d9dlc0000gn/T/tmprz8zb0n0\n",
      "04/22/2019 16:17:34 - INFO -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "04/22/2019 16:17:52 - INFO -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/22/2019 16:17:52 - INFO -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "04/22/2019 16:18:01 - INFO -   loaded best model\n",
      "04/22/2019 16:18:01 - INFO -   getting dev/test data\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|▏         | 1/74 [00:50<1:01:16, 50.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|▎         | 2/74 [01:35<58:36, 48.84s/it]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▍         | 3/74 [02:22<57:05, 48.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 4/74 [03:04<53:59, 46.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 5/74 [03:47<52:15, 45.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 6/74 [04:30<50:41, 44.73s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▉         | 7/74 [05:11<48:30, 43.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█         | 8/74 [05:51<46:51, 42.60s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 9/74 [06:32<45:24, 41.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▎        | 10/74 [07:12<44:09, 41.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▍        | 11/74 [07:52<43:01, 40.98s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▌        | 12/74 [08:34<42:44, 41.36s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 13/74 [09:22<44:10, 43.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▉        | 14/74 [10:08<43:57, 43.96s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 15/74 [10:51<42:59, 43.72s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 16/74 [11:31<41:15, 42.69s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 17/74 [12:11<39:54, 42.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▍       | 18/74 [12:54<39:16, 42.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▌       | 19/74 [13:35<38:18, 41.78s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 20/74 [14:16<37:25, 41.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 21/74 [14:58<36:56, 41.82s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|██▉       | 22/74 [15:39<35:51, 41.37s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███       | 23/74 [16:19<34:49, 40.97s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▏      | 24/74 [17:03<34:59, 41.99s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▍      | 25/74 [17:57<37:20, 45.72s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▌      | 26/74 [18:47<37:37, 47.03s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▋      | 27/74 [19:32<36:14, 46.27s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 28/74 [20:19<35:44, 46.61s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▉      | 29/74 [21:07<35:12, 46.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|████      | 30/74 [21:59<35:30, 48.42s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▏     | 31/74 [22:45<34:15, 47.79s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 32/74 [23:39<34:40, 49.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▍     | 33/74 [24:22<32:33, 47.65s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▌     | 34/74 [25:05<30:53, 46.33s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|████▋     | 35/74 [25:49<29:29, 45.36s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|████▊     | 36/74 [26:32<28:19, 44.73s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 37/74 [27:16<27:24, 44.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████▏    | 38/74 [28:01<26:52, 44.79s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 39/74 [28:44<25:50, 44.29s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████▍    | 40/74 [29:29<25:11, 44.46s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 41/74 [30:12<24:16, 44.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 42/74 [30:56<23:25, 43.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████▊    | 43/74 [31:41<22:49, 44.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▉    | 44/74 [32:22<21:40, 43.34s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████    | 45/74 [33:02<20:27, 42.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▏   | 46/74 [33:42<19:24, 41.60s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████▎   | 47/74 [34:22<18:30, 41.12s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▍   | 48/74 [35:02<17:38, 40.72s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|██████▌   | 49/74 [35:42<16:51, 40.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 50/74 [36:22<16:07, 40.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▉   | 51/74 [37:02<15:30, 40.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████   | 52/74 [37:42<14:47, 40.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▏  | 53/74 [38:23<14:08, 40.39s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 54/74 [39:04<13:35, 40.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|███████▍  | 55/74 [39:46<12:57, 40.94s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|███████▌  | 56/74 [40:26<12:12, 40.68s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 57/74 [41:16<12:19, 43.48s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 58/74 [42:13<12:38, 47.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|███████▉  | 59/74 [43:03<12:03, 48.24s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████  | 60/74 [43:44<10:47, 46.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▏ | 61/74 [44:29<09:53, 45.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████▍ | 62/74 [45:13<09:03, 45.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 63/74 [46:00<08:24, 45.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▋ | 64/74 [46:47<07:40, 46.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 65/74 [47:32<06:53, 45.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▉ | 66/74 [48:18<06:07, 45.91s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████ | 67/74 [49:00<05:11, 44.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 68/74 [49:40<04:19, 43.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████▎| 69/74 [50:20<03:31, 42.29s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▍| 70/74 [51:00<02:46, 41.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▌| 71/74 [51:40<02:03, 41.15s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████▋| 72/74 [52:21<01:22, 41.25s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████▊| 73/74 [53:01<00:40, 40.83s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 74/74 [53:30<00:00, 37.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.85443   0.89109   0.87237       909\n",
      "           1    0.84791   0.73355   0.78660       912\n",
      "           2    0.38055   0.46272   0.41763       389\n",
      "\n",
      "   micro avg    0.75068   0.75068   0.75068      2210\n",
      "   macro avg    0.69430   0.69579   0.69220      2210\n",
      "weighted avg    0.76833   0.75068   0.75693      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enter your bake-off assessment code in this cell. \n",
    "# Please do not remove this comment.\n",
    "\n",
    "## Evaluation: Evaluate using best model\n",
    "args = dotdict({\"data_dir\": './data/bert/', \n",
    "                \"bert_model\": \"bert-large-cased\",\n",
    "                \"output_dir\": \"./models/\",\n",
    "                \"model_save_pth\": \"./models/bert_classification.pth\",\n",
    "                \"seed\": 28,\n",
    "                \"train_batch_size\": 30,\n",
    "                \"num_train_epochs\": 8,\n",
    "                \"eval_batch_size\": 30,\n",
    "                \"do_lower_case\": False,\n",
    "                \"do_train\": True,\n",
    "                \"do_eval\": True,\n",
    "                \"max_seq_length\": 100,\n",
    "                \"gradient_accumulation_steps\": 1,\n",
    "                \"local_rank\": -1,\n",
    "                \"warmup_proportion\": 0.1,\n",
    "                \"fp16\": False,\n",
    "                \"cache_dir\": \"./tmp/\",\n",
    "                \"learning_rate\": 9E-6,\n",
    "                \"do_train_eval\": False})\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349\n",
    "logger.info(\"device is \" + str(device))\n",
    "n_gpu = torch.cuda.device_count()\n",
    "args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "processor = CondProcessor()\n",
    "label_list = processor.get_labels()\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "num_labels = 3\n",
    "\n",
    "# Prepare model\n",
    "model = BertForSequenceClassification.from_pretrained(args.bert_model,\n",
    "          cache_dir=args.cache_dir,\n",
    "          num_labels = num_labels)\n",
    "model.load_state_dict(torch.load('models/pytorch_model.bin', map_location=device))\n",
    "logging.info(\"loaded best model\")\n",
    "\n",
    "eval_examples = processor.get_test_examples() # change this line to `get_test_examples` for test dataset evaluation\n",
    "eval_features = convert_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "preds = []\n",
    "labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "    labels.append(label_ids)\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "print(classification_report(np.concatenate(labels), np.concatenate(preds), digits=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your macro-average F1 value as reported by the code above. \n",
    "# Please enter only a number between 0 and 1 inclusive.\n",
    "# Please do not remove this comment.\n",
    "0.69220"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
